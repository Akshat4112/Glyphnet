diff --git a/code/ImageGeneration.py b/code/ImageGeneration.py
index a69b5a7be..6441c9629 100755
--- a/code/ImageGeneration.py
+++ b/code/ImageGeneration.py
@@ -31,7 +31,7 @@ except OSError as e:
     print('Fake Directory Exists!')
 
 
-data = pd.read_csv(file_path, nrows=200000)
+data = pd.read_csv(file_path, nrows=2000)
 
 print(len(data))
 
@@ -93,13 +93,13 @@ if __name__ == '__main__':
     processes_1 = []
     processes_2 = []
 
-    p1 = multiprocessing.Process(target=multiprocessing_func_1, args=(data.iloc[:500000, 0],))
+    p1 = multiprocessing.Process(target=multiprocessing_func_1, args=(data.iloc[:5000, 0],))
     processes_1.append(p1)
-    p2 = multiprocessing.Process(target=multiprocessing_func_3, args=(data.iloc[500000:1000000, 0],))
+    p2 = multiprocessing.Process(target=multiprocessing_func_3, args=(data.iloc[5000:1000, 0],))
     processes_1.append(p2)
-    p3 = multiprocessing.Process(target=multiprocessing_func_5, args=(data.iloc[1000000:1500000, 0],))
+    p3 = multiprocessing.Process(target=multiprocessing_func_5, args=(data.iloc[1000:1500, 0],))
     processes_1.append(p3)
-    p4 = multiprocessing.Process(target=multiprocessing_func_7, args=(data.iloc[1500000:, 0],))
+    p4 = multiprocessing.Process(target=multiprocessing_func_7, args=(data.iloc[1500:, 0],))
     processes_1.append(p4)
     p1.start()
     p2.start()
@@ -109,13 +109,13 @@ if __name__ == '__main__':
     for process in processes_1:
         process.join()
             
-    p1 = multiprocessing.Process(target=multiprocessing_func_2, args=(data.iloc[:500000, 1],))
+    p1 = multiprocessing.Process(target=multiprocessing_func_2, args=(data.iloc[:500, 1],))
     processes_2.append(p1)
-    p2 = multiprocessing.Process(target=multiprocessing_func_4, args=(data.iloc[500000:1000000, 1],))
+    p2 = multiprocessing.Process(target=multiprocessing_func_4, args=(data.iloc[500:1000, 1],))
     processes_2.append(p2)
-    p3 = multiprocessing.Process(target=multiprocessing_func_6, args=(data.iloc[1000000:1500000, 1],))
+    p3 = multiprocessing.Process(target=multiprocessing_func_6, args=(data.iloc[1000:1500, 1],))
     processes_2.append(p3)
-    p4 = multiprocessing.Process(target=multiprocessing_func_8, args=(data.iloc[1500000:, 1],))
+    p4 = multiprocessing.Process(target=multiprocessing_func_8, args=(data.iloc[1500:, 1],))
     processes_2.append(p4)
     p1.start()
     p2.start()
diff --git a/code/dataGeneration.py b/code/dataGeneration.py
index 48fea5a10..001d06c7d 100755
--- a/code/dataGeneration.py
+++ b/code/dataGeneration.py
@@ -92,10 +92,10 @@ def homo_gen_2(domain):
 domain_file = os.path.join(path_arg, "domains_final.txt")
 
 with open(domain_file, "r") as f:
-	domains_1 = f.read().splitlines()[:100000]
+	domains_1 = f.read().splitlines()[:1000]
 
 with open(domain_file, "r") as f:
-	domains_2 = f.read().splitlines()[100000:200000]
+	domains_2 = f.read().splitlines()[1000:2000]
 
 
 homo = []
diff --git a/code/train.py b/code/train.py
index 34139f8cc..699042cfa 100644
--- a/code/train.py
+++ b/code/train.py
@@ -1,3 +1,5 @@
+import random
+import numpy as np
 from matplotlib import pyplot
 from tensorflow.keras.utils import to_categorical
 from tensorflow.keras import optimizers
@@ -11,18 +13,33 @@ from tensorflow.keras import Model
 import tensorflow as tf
 import wandb
 from wandb.keras import WandbCallback
-
 import argparse
 import os
 from datetime import datetime
 from attentionModule import attach_attention_module
 import matplotlib.pyplot as plt
 
+from sklearn.metrics import confusion_matrix
+from sklearn.metrics import accuracy_score
+from sklearn.metrics import precision_score
+from sklearn.metrics import recall_score
+from sklearn.metrics import f1_score
+from sklearn.metrics import cohen_kappa_score
+from sklearn.metrics import roc_auc_score
+from sklearn.metrics import confusion_matrix
+
+
+#Set the random seeds
+os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
+random.seed(hash("setting random seeds") % 2**32 -1)
+np.random.seed(hash("improves reproducibility")% 2**32 -1)
+tf.random.set_seed(hash("by removing stochasticity")% 2**32 -1)
+
+
+# print(device_lib.list_local_devices())
+
 class NeuralNetwork:
   def __init__(self) -> None:
-    wandb.init(project="HomoglyphDetection", entity="robofied")
-    print(device_lib.list_local_devices())
-    # print(K._get_available_gpus())
     pass
 
   def DataGenerator(self, train_dir, valid_dir):
@@ -30,47 +47,54 @@ class NeuralNetwork:
     self.valid_dir = valid_dir
 
     # create data generator
-    datagen = ImageDataGenerator(rescale=1./255)
+    self.datagen = ImageDataGenerator(rescale=1./255)
 
     # prepare iterator
     print("Before datagen..")
-    self.train_it = datagen.flow_from_directory(train_dir,class_mode='binary', batch_size=1, target_size=(256, 256))
+    self.train_it = self.datagen.flow_from_directory(train_dir,class_mode='binary', batch_size=1, target_size=(256, 256))
     print("Datagen completed..")
-    self.validation_it = datagen.flow_from_directory(valid_dir,class_mode='binary', batch_size=1, target_size=(256, 256))
-    
-  def SimpleCNN(self):
-
-    wandb.config = {"learning_rate": 1e-4,
-                    "epochs": 30,
-                    "batch_size": 64}
-
-    model = models.Sequential()
-    model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(256, 256, 3)))
-    model.add(layers.MaxPool2D(2, 2))
-    model.add(layers.BatchNormalization())
-    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
-    model.add(layers.MaxPool2D(2, 2))
-    model.add(layers.BatchNormalization())
-    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
-    model.add(layers.MaxPool2D(2, 2))
-    model.add(layers.Conv2D(128, (3, 3), activation='relu'))
-    model.add(layers.MaxPool2D(2, 2))
-    model.add(layers.Flatten())
-    model.add(layers.Dense(128, activation='relu'))
-    model.add(layers.Dense(1, activation='sigmoid'))
-
+    self.validation_it = self.datagen.flow_from_directory(valid_dir,class_mode='binary', batch_size=1, target_size=(256, 256))
     
-    model.compile(loss='binary_crossentropy',
-                  optimizer=optimizers.RMSprop(lr=1e-4),
+  def SimpleCNN(self, config):
+    self.config = config
+    self.model = models.Sequential()
+    self.model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(256, 256, 3)))
+    self.model.add(layers.MaxPool2D(2, 2))
+    self.model.add(layers.BatchNormalization())
+    self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))
+    self.model.add(layers.MaxPool2D(2, 2))
+    self.model.add(layers.BatchNormalization())
+    self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))
+    self.model.add(layers.MaxPool2D(2, 2))
+    self.model.add(layers.Conv2D(128, (3, 3), activation='relu'))
+    self.model.add(layers.MaxPool2D(2, 2))
+    self.model.add(layers.Flatten())
+    self.model.add(layers.Dense(128, activation='relu'))
+    self.model.add(layers.Dense(1, activation='sigmoid'))
+    self.model.summary()
+    self.model.compile(loss='binary_crossentropy',
+                  optimizer=optimizers.RMSprop(lr=self.config['learning_rate']),
                   metrics=['acc'])
+    
     callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
-    self.history = model.fit(self.train_it,validation_data=self.validation_it, steps_per_epoch=50, epochs=50, batch_size=64, verbose=1, callbacks=[WandbCallback(), callback])
+    
+    WandbCallback(monitor="vall_loss", save_mode=(True), log_weights=(True))
+
+    self.history = self.model.fit(self.train_it,
+                                  validation_data=self.validation_it, 
+                                  steps_per_epoch=config['steps_per_epoch'], 
+                                  epochs=self.config['epochs'], 
+                                  batch_size=self.config['batch_size'], 
+                                  verbose=1, 
+                                  callbacks=[WandbCallback()])
     
     # save model
     self.name = '../models/modelSimpleCNN' + str(datetime.now()) + '.h5'
-    model.save(self.name)
+    self.model.save(self.name)
+
+  def AttentionCNN(self, config):
+    self.config = config
 
-  def AttentionCNN(self):
     inputs = Input(shape=(256, 256, 3))
 
     x = layers.Rescaling(1.0 / 255)(inputs)
@@ -96,20 +120,28 @@ class NeuralNetwork:
     x = layers.Dense(128, activation='relu')(x)
 
     outputs = layers.Dense(1, activation='sigmoid')(x)
-    model = Model(inputs, outputs)
-
-    model.compile(loss='binary_crossentropy',
-                  optimizer=optimizers.RMSprop(lr=1e-4),
+    self.model = Model(inputs, outputs)
+    
+    self.model.summary()
+    self.model.compile(loss='binary_crossentropy',
+                  optimizer=optimizers.RMSprop(lr=self.config['learning_rate']),
                   metrics=['acc'])
+
     callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
+    WandbCallback(monitor="vall_loss", save_mode=(True), log_weights=(True))
+
+    self.history = self.model.fit(self.train_it,
+                                  validation_data=self.validation_it, 
+                                  steps_per_epoch=config['steps_per_epoch'], 
+                                  epochs=self.config['epochs'], 
+                                  batch_size=self.config['batch_size'], 
+                                  verbose=1, 
+                                  callbacks=[WandbCallback()])
 
-    self.history = model.fit(self.train_it,validation_data=self.validation_it, steps_per_epoch=500, epochs=50, batch_size=64, verbose=1, callbacks=[WandbCallback(), callback])
-    
     # save model
     self.name = '../models/modelAttentionCNN' + str(datetime.now()) + '.h5'
-    model.save(self.name)
+    self.model.save(self.name)
 
-    return None
 
   def plotGraphs(self):
     history = self.history
@@ -132,39 +164,85 @@ class NeuralNetwork:
     loss_fig_name = acc_fig_name = '../figures/' + '_loss_' + self.name[10:-1] + '.png'
     plt.savefig(loss_fig_name)
     
-  # def Evaluation():
-  #       # fit model
-
-  #   print("Evalutaing model..")
-
-  #   # prepare iterator
-  #   print("Before datagen..")
-  #   test_dir = os.path.join(path_arg, "final_test")
-  #   test_it = datagen.flow_from_directory(test_dir, class_mode='binary', batch_size=1, target_size=(256, 256))
-  #   print("Datagen completed..")
-
-  #   # evaluate model
-  #   print("Evaluating Model..")
+  def Evaluation(self):
+    # create data generator
+    print("Evalutaing model..")
+    self.test_it = self.datagen.flow_from_directory("../data/test", class_mode='binary', batch_size=1, target_size=(256, 256))
+    
+    train_loss, train_acc = self.model.evaluate(self.train_it, steps=10, verbose=0)
+    test_loss, test_acc = self.model.evaluate(self.test_it, steps=10, verbose=0)
+    
+    print('Training Loss is: ', train_loss)
+    print('Training Accuracy is: %.3f' % (train_acc * 100.0))
 
-  #   _, acc, f1_score, precision, recall = model.evaluate_generator(test_it, steps=500, verbose=0)
-  #   print('> %.3f' % (acc * 100.0))
+    print('Test Loss is: ', test_loss)
+    print('Test Accuracy is: %.3f' % (test_acc * 100.0))
 
-  #   print("Computing Precision and Recall for Classification.")
+    true_labels = self.test_it.classes
+    predictions = self.model.predict(self.test_it)
+    y_true = true_labels
+    y_pred = np.array([np.argmax(x) for x in predictions])
+    
+    accuracy = accuracy_score(y_true, y_pred)
+    print('Accuracy: %f' % accuracy)
+    precision = precision_score(y_true, y_pred)
+    print('Precision: %f' % precision)
+    recall = recall_score(y_true, y_pred)
+    print('Recall: %f' % recall)
+    f1 = f1_score(y_true, y_pred)
+    print('F1 score: %f' % f1)
+    kappa = cohen_kappa_score(y_true, y_pred)
+    print('Cohens kappa: %f' % kappa)
+    auc = roc_auc_score(y_true, predictions)
+    print('ROC AUC: %f' % auc)
+    matrix = confusion_matrix(y_true, y_pred)
+    print(matrix)
+
+    wandb.log({'Accuracy': accuracy, 
+              'Precision': precision, 
+              'Recall': recall, 
+              'f1-score': f1, 
+              'kappa': kappa, 
+              'auc': auc, 
+              'confusion_matrix': matrix})
+
+
+run = wandb.init(project="HomoglyphDetection", entity="robofied")
 
-  #   return None
 
 obj = NeuralNetwork()
-obj.DataGenerator('../data/train', '../data/valid')
 
+obj.DataGenerator('../data/train', '../data/valid')
 print("Data Generation Completed")
 
-obj.SimpleCNN()
+config = wandb.config = {"learning_rate": 1e-4,
+                          "epochs": 3,
+                          "steps_per_epoch":50,
+                          "batch_size": 16,
+                          "architecture":"Simple CNN",
+                          "dataset":"Glyphnet Dataset",}
+
+obj.SimpleCNN(config)
 obj.plotGraphs()
+obj.Evaluation()
+print("Simple CNN Experiment Completed")
+run.finish()
+
+
+
+run = wandb.init(reinit=True)
+
+config = wandb.config = {"learning_rate": 1e-4,
+                          "epochs": 3,
+                          "steps_per_epoch":50,
+                          "batch_size": 16,
+                          "architecture":"Attention CNN",
+                          "dataset":"Glyphnet Dataset",}
 
-print("Simple CNN Completed. ")
-print("Attention CNN Started ")
 
-obj.AttentionCNN()
+obj.AttentionCNN(config)
 obj.plotGraphs()
+obj.Evaluation()
+wandb.finish()
 
-print("Attention CNN Completed")
\ No newline at end of file
+print("Attention CNN Experiment Completed")
\ No newline at end of file
diff --git a/code/wandb/debug-internal.log b/code/wandb/debug-internal.log
index 4215f37a1..52b53b774 120000
--- a/code/wandb/debug-internal.log
+++ b/code/wandb/debug-internal.log
@@ -1 +1 @@
-run-20220808_012655-2dgfuw9r/logs/debug-internal.log
\ No newline at end of file
+run-20220917_132450-2puj4853/logs/debug-internal.log
\ No newline at end of file
diff --git a/code/wandb/debug.log b/code/wandb/debug.log
index c4a057813..899bb65db 120000
--- a/code/wandb/debug.log
+++ b/code/wandb/debug.log
@@ -1 +1 @@
-run-20220808_012655-2dgfuw9r/logs/debug.log
\ No newline at end of file
+run-20220917_132450-2puj4853/logs/debug.log
\ No newline at end of file
diff --git a/code/wandb/latest-run b/code/wandb/latest-run
index 7054dc890..b713f02ea 120000
--- a/code/wandb/latest-run
+++ b/code/wandb/latest-run
@@ -1 +1 @@
-run-20220808_012655-2dgfuw9r
\ No newline at end of file
+run-20220917_132450-2puj4853
\ No newline at end of file
