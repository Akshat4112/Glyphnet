diff --git a/code/ImageGeneration.py b/code/ImageGeneration.py
index a69b5a7be..6441c9629 100755
--- a/code/ImageGeneration.py
+++ b/code/ImageGeneration.py
@@ -31,7 +31,7 @@ except OSError as e:
     print('Fake Directory Exists!')
 
 
-data = pd.read_csv(file_path, nrows=200000)
+data = pd.read_csv(file_path, nrows=2000)
 
 print(len(data))
 
@@ -93,13 +93,13 @@ if __name__ == '__main__':
     processes_1 = []
     processes_2 = []
 
-    p1 = multiprocessing.Process(target=multiprocessing_func_1, args=(data.iloc[:500000, 0],))
+    p1 = multiprocessing.Process(target=multiprocessing_func_1, args=(data.iloc[:5000, 0],))
     processes_1.append(p1)
-    p2 = multiprocessing.Process(target=multiprocessing_func_3, args=(data.iloc[500000:1000000, 0],))
+    p2 = multiprocessing.Process(target=multiprocessing_func_3, args=(data.iloc[5000:1000, 0],))
     processes_1.append(p2)
-    p3 = multiprocessing.Process(target=multiprocessing_func_5, args=(data.iloc[1000000:1500000, 0],))
+    p3 = multiprocessing.Process(target=multiprocessing_func_5, args=(data.iloc[1000:1500, 0],))
     processes_1.append(p3)
-    p4 = multiprocessing.Process(target=multiprocessing_func_7, args=(data.iloc[1500000:, 0],))
+    p4 = multiprocessing.Process(target=multiprocessing_func_7, args=(data.iloc[1500:, 0],))
     processes_1.append(p4)
     p1.start()
     p2.start()
@@ -109,13 +109,13 @@ if __name__ == '__main__':
     for process in processes_1:
         process.join()
             
-    p1 = multiprocessing.Process(target=multiprocessing_func_2, args=(data.iloc[:500000, 1],))
+    p1 = multiprocessing.Process(target=multiprocessing_func_2, args=(data.iloc[:500, 1],))
     processes_2.append(p1)
-    p2 = multiprocessing.Process(target=multiprocessing_func_4, args=(data.iloc[500000:1000000, 1],))
+    p2 = multiprocessing.Process(target=multiprocessing_func_4, args=(data.iloc[500:1000, 1],))
     processes_2.append(p2)
-    p3 = multiprocessing.Process(target=multiprocessing_func_6, args=(data.iloc[1000000:1500000, 1],))
+    p3 = multiprocessing.Process(target=multiprocessing_func_6, args=(data.iloc[1000:1500, 1],))
     processes_2.append(p3)
-    p4 = multiprocessing.Process(target=multiprocessing_func_8, args=(data.iloc[1500000:, 1],))
+    p4 = multiprocessing.Process(target=multiprocessing_func_8, args=(data.iloc[1500:, 1],))
     processes_2.append(p4)
     p1.start()
     p2.start()
diff --git a/code/dataGeneration.py b/code/dataGeneration.py
index 48fea5a10..001d06c7d 100755
--- a/code/dataGeneration.py
+++ b/code/dataGeneration.py
@@ -92,10 +92,10 @@ def homo_gen_2(domain):
 domain_file = os.path.join(path_arg, "domains_final.txt")
 
 with open(domain_file, "r") as f:
-	domains_1 = f.read().splitlines()[:100000]
+	domains_1 = f.read().splitlines()[:1000]
 
 with open(domain_file, "r") as f:
-	domains_2 = f.read().splitlines()[100000:200000]
+	domains_2 = f.read().splitlines()[1000:2000]
 
 
 homo = []
diff --git a/code/train.py b/code/train.py
index 34139f8cc..52601ce44 100644
--- a/code/train.py
+++ b/code/train.py
@@ -1,3 +1,5 @@
+import random
+import numpy as np
 from matplotlib import pyplot
 from tensorflow.keras.utils import to_categorical
 from tensorflow.keras import optimizers
@@ -11,18 +13,24 @@ from tensorflow.keras import Model
 import tensorflow as tf
 import wandb
 from wandb.keras import WandbCallback
-
 import argparse
 import os
 from datetime import datetime
 from attentionModule import attach_attention_module
 import matplotlib.pyplot as plt
 
+
+#Set the random seeds
+os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
+random.seed(hash("setting random seeds") % 2**32 -1)
+np.random.seed(hash("improves reproducibility")% 2**32 -1)
+tf.random.set_seed(hash("by removing stochasticity")% 2**32 -1)
+
+wandb.init(project="HomoglyphDetection", entity="robofied")
+# print(device_lib.list_local_devices())
+
 class NeuralNetwork:
   def __init__(self) -> None:
-    wandb.init(project="HomoglyphDetection", entity="robofied")
-    print(device_lib.list_local_devices())
-    # print(K._get_available_gpus())
     pass
 
   def DataGenerator(self, train_dir, valid_dir):
@@ -30,45 +38,40 @@ class NeuralNetwork:
     self.valid_dir = valid_dir
 
     # create data generator
-    datagen = ImageDataGenerator(rescale=1./255)
+    self.datagen = ImageDataGenerator(rescale=1./255)
 
     # prepare iterator
     print("Before datagen..")
-    self.train_it = datagen.flow_from_directory(train_dir,class_mode='binary', batch_size=1, target_size=(256, 256))
+    self.train_it = self.datagen.flow_from_directory(train_dir,class_mode='binary', batch_size=1, target_size=(256, 256))
     print("Datagen completed..")
-    self.validation_it = datagen.flow_from_directory(valid_dir,class_mode='binary', batch_size=1, target_size=(256, 256))
-    
-  def SimpleCNN(self):
-
-    wandb.config = {"learning_rate": 1e-4,
-                    "epochs": 30,
-                    "batch_size": 64}
-
-    model = models.Sequential()
-    model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(256, 256, 3)))
-    model.add(layers.MaxPool2D(2, 2))
-    model.add(layers.BatchNormalization())
-    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
-    model.add(layers.MaxPool2D(2, 2))
-    model.add(layers.BatchNormalization())
-    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
-    model.add(layers.MaxPool2D(2, 2))
-    model.add(layers.Conv2D(128, (3, 3), activation='relu'))
-    model.add(layers.MaxPool2D(2, 2))
-    model.add(layers.Flatten())
-    model.add(layers.Dense(128, activation='relu'))
-    model.add(layers.Dense(1, activation='sigmoid'))
-
+    self.validation_it = self.datagen.flow_from_directory(valid_dir,class_mode='binary', batch_size=1, target_size=(256, 256))
     
-    model.compile(loss='binary_crossentropy',
-                  optimizer=optimizers.RMSprop(lr=1e-4),
+  def SimpleCNN(self, config):
+    self.config = config
+    self.model = models.Sequential()
+    self.model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(256, 256, 3)))
+    self.model.add(layers.MaxPool2D(2, 2))
+    self.model.add(layers.BatchNormalization())
+    self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))
+    self.model.add(layers.MaxPool2D(2, 2))
+    self.model.add(layers.BatchNormalization())
+    self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))
+    self.model.add(layers.MaxPool2D(2, 2))
+    self.model.add(layers.Conv2D(128, (3, 3), activation='relu'))
+    self.model.add(layers.MaxPool2D(2, 2))
+    self.model.add(layers.Flatten())
+    self.model.add(layers.Dense(128, activation='relu'))
+    self.model.add(layers.Dense(1, activation='sigmoid'))
+    self.model.summary()
+    self.model.compile(loss='binary_crossentropy',
+                  optimizer=optimizers.RMSprop(lr=self.config['learning_rate']),
                   metrics=['acc'])
     callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
-    self.history = model.fit(self.train_it,validation_data=self.validation_it, steps_per_epoch=50, epochs=50, batch_size=64, verbose=1, callbacks=[WandbCallback(), callback])
+    self.history = self.model.fit(self.train_it,validation_data=self.validation_it, steps_per_epoch=50, epochs=self.config['epochs'], batch_size=self.config['batch_size'], verbose=1, callbacks=[WandbCallback(), callback])
     
     # save model
     self.name = '../models/modelSimpleCNN' + str(datetime.now()) + '.h5'
-    model.save(self.name)
+    self.model.save(self.name)
 
   def AttentionCNN(self):
     inputs = Input(shape=(256, 256, 3))
@@ -97,6 +100,7 @@ class NeuralNetwork:
 
     outputs = layers.Dense(1, activation='sigmoid')(x)
     model = Model(inputs, outputs)
+    
 
     model.compile(loss='binary_crossentropy',
                   optimizer=optimizers.RMSprop(lr=1e-4),
@@ -132,39 +136,36 @@ class NeuralNetwork:
     loss_fig_name = acc_fig_name = '../figures/' + '_loss_' + self.name[10:-1] + '.png'
     plt.savefig(loss_fig_name)
     
-  # def Evaluation():
-  #       # fit model
-
-  #   print("Evalutaing model..")
-
-  #   # prepare iterator
-  #   print("Before datagen..")
-  #   test_dir = os.path.join(path_arg, "final_test")
-  #   test_it = datagen.flow_from_directory(test_dir, class_mode='binary', batch_size=1, target_size=(256, 256))
-  #   print("Datagen completed..")
-
-  #   # evaluate model
-  #   print("Evaluating Model..")
-
-  #   _, acc, f1_score, precision, recall = model.evaluate_generator(test_it, steps=500, verbose=0)
-  #   print('> %.3f' % (acc * 100.0))
+  def Evaluation(self):
+    # create data generator
+    print("Evalutaing model..")
+    test_it = self.datagen.flow_from_directory("../data/test", class_mode='binary', batch_size=1, target_size=(256, 256))
 
-  #   print("Computing Precision and Recall for Classification.")
+    loss, acc = self.model.evaluate(test_it, steps=10, verbose=0)
+    print('Loss is: ', loss)
+    print('> %.3f' % (acc * 100.0))
 
-  #   return None
+    print("Computing Precision and Recall for Classification.")
+    
+    
 
 obj = NeuralNetwork()
 obj.DataGenerator('../data/train', '../data/valid')
-
 print("Data Generation Completed")
-
-obj.SimpleCNN()
+conbfig = wandb.config = {"learning_rate": 1e-4,
+                          "epochs": 1,
+                          "batch_size": 64,
+                          "architecture":"Simple CNN",
+                          "dataset":"Glyphnet Dataset",}
+obj.SimpleCNN(conbfig)
 obj.plotGraphs()
+obj.Evaluation()
 
 print("Simple CNN Completed. ")
-print("Attention CNN Started ")
 
-obj.AttentionCNN()
-obj.plotGraphs()
+# print("Attention CNN Started ")
+
+# obj.AttentionCNN()
+# obj.plotGraphs()
 
-print("Attention CNN Completed")
\ No newline at end of file
+# print("Attention CNN Completed")
\ No newline at end of file
diff --git a/code/wandb/debug-internal.log b/code/wandb/debug-internal.log
index 4215f37a1..319b7247a 120000
--- a/code/wandb/debug-internal.log
+++ b/code/wandb/debug-internal.log
@@ -1 +1 @@
-run-20220808_012655-2dgfuw9r/logs/debug-internal.log
\ No newline at end of file
+run-20220917_115847-340ljiib/logs/debug-internal.log
\ No newline at end of file
diff --git a/code/wandb/debug.log b/code/wandb/debug.log
index c4a057813..044628df9 120000
--- a/code/wandb/debug.log
+++ b/code/wandb/debug.log
@@ -1 +1 @@
-run-20220808_012655-2dgfuw9r/logs/debug.log
\ No newline at end of file
+run-20220917_115847-340ljiib/logs/debug.log
\ No newline at end of file
diff --git a/code/wandb/latest-run b/code/wandb/latest-run
index 7054dc890..81c17842e 120000
--- a/code/wandb/latest-run
+++ b/code/wandb/latest-run
@@ -1 +1 @@
-run-20220808_012655-2dgfuw9r
\ No newline at end of file
+run-20220917_115847-340ljiib
\ No newline at end of file
